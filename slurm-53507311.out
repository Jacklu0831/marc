UNMASK TOKENS IN CURRENT MODE
Number of train tasks:  80
Ignoring lora_to_output for llama3_2
{'batch_size': 2,
 'checkpointer': {'_component_': 'torchtune.training.FullModelHFCheckpointer', 'checkpoint_dir': 'downloaded_models/meta-llama/Llama-3.2-1B-Instruct', 'checkpoint_files': ['model.safetensors'], 'recipe_checkpoint': None, 'output_dir': 'train_outputs/1114_llama1b_ttt/', 'model_type': 'LLAMA3'},
 'compile': False,
 'dataset': {'_component_': 'torchtune.datasets.arc_dataset', 'source': 'data/dummy/', 'train_on_input': False, 'unmask_outputs': True},
 'device': 'cuda',
 'dtype': 'bf16',
 'enable_activation_checkpointing': True,
 'enable_activation_offloading': False,
 'epochs': 2,
 'gradient_accumulation_steps': 1,
 'log_every_n_steps': 1,
 'log_peak_memory_stats': False,
 'loss': {'_component_': 'torch.nn.CrossEntropyLoss'},
 'lr_scheduler': {'_component_': 'torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup', 'num_warmup_steps': 5},
 'max_steps_per_epoch': None,
 'metric_logger': {'_component_': 'torchtune.training.metric_logging.DiskLogger', 'log_dir': '${output_dir}'},
 'model': {'_component_': 'torchtune.models.llama3_2.lora_llama3_2_1b', 'lora_attn_modules': ['q_proj', 'v_proj'], 'apply_lora_to_mlp': True, 'lora_rank': 128, 'lora_alpha': 16.0, 'lora_dropout': 0.0},
 'optimizer': {'_component_': 'torch.optim.AdamW', 'fused': True, 'weight_decay': 0.01, 'lr': 5e-05},
 'output_dir': 'train/outputs/test',
 'profiler': {'_component_': 'torchtune.training.setup_torch_profiler', 'enabled': False, 'output_dir': '${output_dir}/profiling_outputs', 'cpu': True, 'cuda': True, 'profile_memory': False, 'with_stack': False, 'record_shapes': True, 'with_flops': False, 'wait_steps': 5, 'warmup_steps': 5, 'active_steps': 2, 'num_cycles': 1},
 'resume_from_checkpoint': False,
 'save_adapter_weights_only': True,
 'seed': 0,
 'shuffle': True,
 'tokenizer': {'_component_': 'torchtune.models.llama3.llama3_tokenizer', 'path': 'downloaded_models/meta-llama/Llama-3.2-1B-Instruct/original/tokenizer.model', 'max_seq_len': None}}
Augmenters to apply:  [Rotate(90), Rotate(270), Rotate(180), Flip(0), Flip(1), Reflect(0, reverse=True), Reflect(1, reverse=True), Reflect(0, reverse=False), Reflect(1, reverse=False), RandomTranslateXY(), Transpose(), IncreaseResolution(2), IncreaseHeight(2), IncreaseWidth(2), Chain([Rotate(90), IncreaseResolution(2)]), Chain([Rotate(270), IncreaseResolution(2)]), Chain([Rotate(180), IncreaseResolution(2)]), Chain([Flip(0), IncreaseResolution(2)]), Chain([Flip(1), IncreaseResolution(2)]), Chain([Transpose(), IncreaseResolution(2)]), Repeat(0, 2), Repeat(1, 2), Repeat(2, 2)] len:  23
Traceback (most recent call last):
  File "/scratch/yl11330/marc/test_time_train.py", line 307, in <module>
    with open(
PermissionError: [Errno 13] Permission denied: 'train_outputs/1114_llama1b_ttt/00576224/td_False_ttd_False_ttdwa_False_ad_True_trd_False.jsonl'
